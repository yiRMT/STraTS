{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "driven-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-record",
   "metadata": {},
   "source": [
    "## Load forecast dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "executive-annual",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86902it [00:00, 822342.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 26/26 [04:08<00:00,  9.55s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_window = 2 # hours\n",
    "obs_windows = range(20, 124, 4)\n",
    "\n",
    "# Read data.\n",
    "data_path = './../mimic_iii_preprocessed.pkl'\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Remove test patients.\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))\n",
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)    \n",
    "    obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'hour':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "fore_op = np.concatenate(fore_op, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]]\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo\n",
    "fore_train_op = fore_op[train_ind]\n",
    "fore_valid_op = fore_op[valid_ind]\n",
    "del fore_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "realistic-control",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15948, (421909, 880), (130714, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rem_sub), fore_train_ip[1].shape, fore_valid_ip[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessory-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = data.groupby('ts_ind').agg({'hour':'max'}).hour\n",
    "# for q in [0.5,0.6,0.7,0.8,0.9, 0.95, 0.99, 1]:\n",
    "#     print (q, m.quantile(q))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(sorted(list(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-indiana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-hanging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "imperial-concern",
   "metadata": {},
   "source": [
    "## Load target dataset into matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stretch-hybrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89624it [00:00, 762209.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17468779it [00:27, 636033.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read data.\n",
    "data_path = './../mimic_iii_preprocessed.pkl'\n",
    "data, oc, train_ind, valid_ind, test_ind = pickle.load(open(data_path, 'rb'))\n",
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_mortality']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print ('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del times_inp, values_inp, varis_inp\n",
    "train_op = y[train_ind]\n",
    "valid_op = y[valid_ind]\n",
    "test_op = y[test_ind]\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de2f5468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11492457377488173"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate((train_op,valid_op,test_op))\n",
    "y.sum()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-collect",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# l = data.groupby('ts_ind').size()\n",
    "# print (sorted(l)[-500:], l.quantile(0.99), len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-vault",
   "metadata": {},
   "source": [
    "## Define metrics and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "familiar-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "    \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-swing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "metallic-forth",
   "metadata": {},
   "source": [
    "## Define model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "subsequent-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = tf_utils.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = tf_utils.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = tf_utils.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-fetish",
   "metadata": {},
   "source": [
    "## Pretrain on forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "temporal-grain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tsaisindhura/anaconda3/envs/strats/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/tsaisindhura/anaconda3/envs/strats/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 880)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 880)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 880)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 880, 50)      6500        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cve (CVE)                       (None, 880, 50)      364         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cve_1 (CVE)                     (None, 880, 50)      364         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 880, 50)      0           embedding[0][0]                  \n",
      "                                                                 cve[0][0]                        \n",
      "                                                                 cve_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 880)          0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer (Transformer)       (None, 880, 50)      39508       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 880, 1)       5200        transformer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          300         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 50)           0           transformer[0][0]                \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           5050        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100)          0           lambda_1[0][0]                   \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 129)          13029       concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 70,315\n",
      "Trainable params: 70,315\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/3200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tsaisindhura/anaconda3/envs/strats/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 20:35:57.217636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-06 20:36:00.668752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:3b:00.0\n",
      "2022-11-06 20:36:00.669982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:5e:00.0\n",
      "2022-11-06 20:36:00.671153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:af:00.0\n",
      "2022-11-06 20:36:00.672329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:d8:00.0\n",
      "2022-11-06 20:36:00.672578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2022-11-06 20:36:00.673873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2022-11-06 20:36:00.675080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2022-11-06 20:36:00.675381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2022-11-06 20:36:00.676841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2022-11-06 20:36:00.677974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2022-11-06 20:36:00.681312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-06 20:36:00.690441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2022-11-06 20:36:00.691455: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2022-11-06 20:36:00.732481: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2022-11-06 20:36:00.741824: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d97f132b80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-06 20:36:00.741880: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-11-06 20:36:01.208312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:3b:00.0\n",
      "2022-11-06 20:36:01.209920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:5e:00.0\n",
      "2022-11-06 20:36:01.211463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 2 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:af:00.0\n",
      "2022-11-06 20:36:01.212999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 3 with properties: \n",
      "name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.77\n",
      "pciBusID: 0000:d8:00.0\n",
      "2022-11-06 20:36:01.213071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2022-11-06 20:36:01.213084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "2022-11-06 20:36:01.213094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
      "2022-11-06 20:36:01.213104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
      "2022-11-06 20:36:01.213114: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
      "2022-11-06 20:36:01.213123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
      "2022-11-06 20:36:01.213134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-06 20:36:01.224987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1, 2, 3\n",
      "2022-11-06 20:36:01.225045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
      "2022-11-06 20:36:01.231789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-06 20:36:01.231805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 2 3 \n",
      "2022-11-06 20:36:01.231811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y Y Y \n",
      "2022-11-06 20:36:01.231814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N Y Y \n",
      "2022-11-06 20:36:01.231817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 2:   Y Y N Y \n",
      "2022-11-06 20:36:01.231820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 3:   Y Y Y N \n",
      "2022-11-06 20:36:01.240903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 46017 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:3b:00.0, compute capability: 7.5)\n",
      "2022-11-06 20:36:01.244526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 46017 MB memory) -> physical GPU (device: 1, name: Quadro RTX 8000, pci bus id: 0000:5e:00.0, compute capability: 7.5)\n",
      "2022-11-06 20:36:01.247780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 46017 MB memory) -> physical GPU (device: 2, name: Quadro RTX 8000, pci bus id: 0000:af:00.0, compute capability: 7.5)\n",
      "2022-11-06 20:36:01.251060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 46017 MB memory) -> physical GPU (device: 3, name: Quadro RTX 8000, pci bus id: 0000:d8:00.0, compute capability: 7.5)\n",
      "2022-11-06 20:36:01.253692: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d97da69850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-06 20:36:01.253709: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 8000, Compute Capability 7.5\n",
      "2022-11-06 20:36:01.253714: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Quadro RTX 8000, Compute Capability 7.5\n",
      "2022-11-06 20:36:01.253717: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Quadro RTX 8000, Compute Capability 7.5\n",
      "2022-11-06 20:36:01.253721: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Quadro RTX 8000, Compute Capability 7.5\n",
      "2022-11-06 20:36:03.708376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
      "0.360151: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:51<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 105s 800us/sample - loss: 9.4982\n",
      "Epoch 0 loss 11.521339502781629 val loss 9.49820749395281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.293775: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:41<00:00,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 9.4233 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 104s 792us/sample - loss: 8.6141\n",
      "Epoch 1 loss 9.397938494384288 val loss 8.61412793928223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.272637: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:40<00:00,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 8.9605 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 790us/sample - loss: 8.2323\n",
      "Epoch 2 loss 8.721755356863142 val loss 8.232326513073104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.263511: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:40<00:00,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 8.5472 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 104s 794us/sample - loss: 8.0077\n",
      "Epoch 3 loss 8.429789003506302 val loss 8.007674853005783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.259029: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:48 - loss: 8.5293 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 789us/sample - loss: 7.8055\n",
      "Epoch 4 loss 8.286408933997155 val loss 7.805498668780764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.252111: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:40<00:00,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 8.5630 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 102s 784us/sample - loss: 7.5663\n",
      "Epoch 5 loss 8.065105390548705 val loss 7.56633391919403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.241777: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 8.3549 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 789us/sample - loss: 7.5176\n",
      "Epoch 6 loss 7.734512438625098 val loss 7.517591603936021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.246859: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 8.2489 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 104s 793us/sample - loss: 7.4988\n",
      "Epoch 7 loss 7.89709732465446 val loss 7.498829790283571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.242240: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 7.9492 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 785us/sample - loss: 7.3384\n",
      "Epoch 8 loss 7.7493439324200155 val loss 7.3384169174207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.244519: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 7.9111 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 7.3625\n",
      "Epoch 9 loss 7.8222547501325606 val loss 7.3624967166496385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.240371: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 7.9424 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 104s 795us/sample - loss: 7.3040\n",
      "Epoch 10 loss 7.689546133950353 val loss 7.304045297222662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.234479: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 7.9328 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 104s 793us/sample - loss: 7.2049\n",
      "Epoch 11 loss 7.501065139845013 val loss 7.2048564822178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.236239: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:53 - loss: 8.7412 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 790us/sample - loss: 7.5654\n",
      "Epoch 12 loss 7.557375007420778 val loss 7.565445181694441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.230722: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 7.7151 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 789us/sample - loss: 7.2046\n",
      "Epoch 13 loss 7.380873156636953 val loss 7.204648953952646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.227410: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 7.7382 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 7.1688\n",
      "Epoch 14 loss 7.274931700155139 val loss 7.168828114157002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.231743: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 7.6852 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 787us/sample - loss: 7.0386\n",
      "Epoch 15 loss 7.413527066856623 val loss 7.0385910169853965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.226465: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.6499 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 785us/sample - loss: 7.1059\n",
      "Epoch 16 loss 7.244686341136694 val loss 7.105876587584029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.228149: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 7.8976 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 790us/sample - loss: 7.0417\n",
      "Epoch 17 loss 7.29855405151844 val loss 7.041663353476379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.228863: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.6673 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 787us/sample - loss: 6.9996\n",
      "Epoch 18 loss 7.3213922334462405 val loss 6.999618385563206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.231413: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 7.4190"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 104s 792us/sample - loss: 6.9867\n",
      "Epoch 19 loss 7.40297484561801 val loss 6.98671518447253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.225285: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 7.4873"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 789us/sample - loss: 6.9652\n",
      "Epoch 20 loss 7.206924572438002 val loss 6.965218824405449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.225058: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 7.6366 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 6.9512\n",
      "Epoch 21 loss 7.199683334454894 val loss 6.951232890461008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.221680: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 7.3418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 786us/sample - loss: 6.9695\n",
      "Epoch 22 loss 7.091597516909242 val loss 6.969499421177693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.226346: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.9217 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 6.9409\n",
      "Epoch 23 loss 7.240893275886774 val loss 6.940916955391696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.222353: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.2472"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 6.8431\n",
      "Epoch 24 loss 7.113132432848215 val loss 6.843078951106262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.221526: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 7.4471"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 6.9772\n",
      "Epoch 25 loss 7.0866992457211015 val loss 6.977194840561094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.219658: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:54 - loss: 7.5216 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 785us/sample - loss: 6.8609\n",
      "Epoch 26 loss 7.026940780133009 val loss 6.860922192973464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.225724: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:48 - loss: 7.2309"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 784us/sample - loss: 6.7968\n",
      "Epoch 27 loss 7.220996360033751 val loss 6.796751034807535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.224280: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 7.4111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 785us/sample - loss: 6.8511\n",
      "Epoch 28 loss 7.174801443591714 val loss 6.85111950017529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.222929: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 7.6106"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 789us/sample - loss: 6.8814\n",
      "Epoch 29 loss 7.131565191075206 val loss 6.881413898902561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.220573: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 7.6571 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 791us/sample - loss: 6.9733\n",
      "Epoch 30 loss 7.056189107000828 val loss 6.973291907664409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.220885: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 7.3046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 790us/sample - loss: 6.8438\n",
      "Epoch 31 loss 7.0661883630603555 val loss 6.843793197195221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.218270: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 7.3460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 786us/sample - loss: 6.7257\n",
      "Epoch 32 loss 6.982513721734286 val loss 6.72565069681523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.218578: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.2980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 787us/sample - loss: 6.8435\n",
      "Epoch 33 loss 6.9923700650781395 val loss 6.843466281267071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.217298: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:49 - loss: 7.0493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 785us/sample - loss: 6.8237\n",
      "Epoch 34 loss 6.951443980559707 val loss 6.823731822468637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.219724: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.1687"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 6.7442\n",
      "Epoch 35 loss 7.029023719504476 val loss 6.7442301494637436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.219798: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.2726"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 786us/sample - loss: 6.6937\n",
      "Epoch 36 loss 7.031399982497096 val loss 6.693660007217773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.216904: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:52 - loss: 7.2559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 6.7983\n",
      "Epoch 37 loss 6.93882612593472 val loss 6.798263793126101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.215454: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:39<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:51 - loss: 7.2562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 102s 784us/sample - loss: 6.9138\n",
      "Epoch 38 loss 6.892449416294694 val loss 6.913782135411593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.216464: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.1311"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 787us/sample - loss: 6.7664\n",
      "Epoch 39 loss 6.9247444553673265 val loss 6.7663977827584345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.215685: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:44 - loss: 7.0467"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 785us/sample - loss: 6.6983\n",
      "Epoch 40 loss 6.8998456051200625 val loss 6.698261646008421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.219329: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:48 - loss: 6.9850"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 788us/sample - loss: 6.7213\n",
      "Epoch 41 loss 7.016408471763134 val loss 6.721296689205268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.216868: 100%|█████████████████████████████████████████████████████████████████████| 3200/3200 [05:38<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    96/130714 [..............................] - ETA: 1:50 - loss: 7.2774"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130714/130714 [==============================] - 103s 787us/sample - loss: 6.8128\n",
      "Epoch 42 loss 6.93767501488328 val loss 6.812803515299831\n"
     ]
    }
   ],
   "source": [
    "lr, batch_size, samples_per_epoch, patience = 0.0005, 32, 102400, 5\n",
    "d, N, he, dropout = 50, 2, 4, 0.2\n",
    "model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "print (fore_model.summary())\n",
    "fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "\n",
    "# Pretrain fore_model.\n",
    "best_val_loss = np.inf\n",
    "N_fore = len(fore_train_op)\n",
    "fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore.h5'\n",
    "\n",
    "for e in range(1000):\n",
    "    e_indices = np.random.choice(range(N_fore), size=samples_per_epoch, replace=False)\n",
    "    e_loss = 0\n",
    "    pbar = tqdm(range(0, len(e_indices), batch_size))\n",
    "    for start in pbar:\n",
    "        ind = e_indices[start:start+batch_size]\n",
    "        e_loss += fore_model.train_on_batch([ip[ind] for ip in fore_train_ip], fore_train_op[ind])\n",
    "        pbar.set_description('%f'%(e_loss/(start+1)))\n",
    "    val_loss = fore_model.evaluate(fore_valid_ip, fore_valid_op, batch_size=batch_size, verbose=1)\n",
    "    print ('Epoch', e, 'loss', e_loss*batch_size/samples_per_epoch, 'val loss', val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        fore_model.save_weights(fore_savepath)\n",
    "        best_epoch = e\n",
    "    if (e-best_epoch)>patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "western-theme",
   "metadata": {},
   "source": [
    "## Train on different % of labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "czech-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5427val_aucs: 0.5119571018937861 0.8871125286554371\n",
      "2879/2879 [==============================] - 13s 5ms/sample - loss: 0.5407\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4494val_aucs: 0.5370546464909055 0.8966410844214093\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4502\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4255val_aucs: 0.5239887382705275 0.8902820691717332\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4244\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3972val_aucs: 0.49576093023751544 0.8724010764477225\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3976\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3729val_aucs: 0.4970823686030347 0.882128974384531\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3730\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3463val_aucs: 0.4987097479781401 0.8638293631017642\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3465\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3173val_aucs: 0.4736964593875629 0.8555765972291438\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3180\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2991val_aucs: 0.4156272586579566 0.82491777135453\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2986\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2724val_aucs: 0.3934758518397917 0.8298813914083524\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2710\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2616val_aucs: 0.41530481732956415 0.8376158676367985\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2628\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2485val_aucs: 0.43054037013262514 0.8377155387222167\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2464\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2118val_aucs: 0.37573778590244455 0.8211302701086415\n",
      "2879/2879 [==============================] - 10s 4ms/sample - loss: 0.2137\n",
      "Test res 0.8743722809599422 0.5291876652914734 0.501460564751704\n",
      "Repeat 1 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5469val_aucs: 0.5991156136817113 0.8918482386574709\n",
      "2879/2879 [==============================] - 14s 5ms/sample - loss: 0.5443\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4468val_aucs: 0.6125132648862537 0.8987773165571042\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4462\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4157val_aucs: 0.5817444995200945 0.8862435998901684\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4135\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3947val_aucs: 0.5657537531390997 0.8861143863162826\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3942\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3811val_aucs: 0.5723891906283859 0.8799121347697575\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3829\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3534val_aucs: 0.5186942035234737 0.86560173146189\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3556\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3171val_aucs: 0.5256643593415831 0.8725308093615234\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3178\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3042val_aucs: 0.536131931634283 0.8714486456802287\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3088\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2755val_aucs: 0.49928420243292637 0.8632435837384717\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2767\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2574val_aucs: 0.49429203661467647 0.8618706895159336\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2567\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2322val_aucs: 0.48475327312067285 0.853827144541534\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2315\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2270val_aucs: 0.47764661487358884 0.8388706733642369\n",
      "2879/2879 [==============================] - 11s 4ms/sample - loss: 0.2266\n",
      "Test res 0.8792260870946824 0.5311110785510709 0.5081184336198663\n",
      "Repeat 2 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5329val_aucs: 0.4938446959243891 0.8477424499152796\n",
      "2879/2879 [==============================] - 14s 5ms/sample - loss: 0.5305\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4424val_aucs: 0.526221259199062 0.8495963321040567\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4429\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4170val_aucs: 0.5492949844101057 0.8495763978869729\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4187\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3743val_aucs: 0.486305037670922 0.8398883683843317\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3738\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3683val_aucs: 0.54970047144195 0.8450513306089903\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3701\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3395val_aucs: 0.4687372870943172 0.8262732981162163\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3391\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3129val_aucs: 0.4582497145779279 0.8223861257849099\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3147\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2877val_aucs: 0.42229283720791316 0.8031496062992125\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2886\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2713val_aucs: 0.4639899250513769 0.8249975082228646\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2704\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2545val_aucs: 0.4675143532806064 0.8294827070666798\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2534\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2252val_aucs: 0.4109817432884968 0.8074553971892754\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2244\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2148val_aucs: 0.4284399558226777 0.8164457290939898\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2153\n",
      "Epoch 13/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.1856val_aucs: 0.4095715905779942 0.7992225655337387\n",
      "2879/2879 [==============================] - 11s 4ms/sample - loss: 0.1869\n",
      "Test res 0.8748304401406529 0.5313563525017719 0.5034079844206426\n",
      "Repeat 3 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5410val_aucs: 0.48277151891733905 0.86534310687496\n",
      "2879/2879 [==============================] - 15s 5ms/sample - loss: 0.5411\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4476val_aucs: 0.48181472738719544 0.869702733314812\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4465\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4207val_aucs: 0.4746716207196405 0.8723527023272711\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4196\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3948val_aucs: 0.46893044056328936 0.8665826085098198\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3964\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3738val_aucs: 0.4221008905691707 0.8671168764558802\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3737\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3491val_aucs: 0.4701971878765569 0.8725877802235377\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3483\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3253val_aucs: 0.4354408620603336 0.8566665954309405\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3246\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3264val_aucs: 0.4062952166583875 0.8598080909537751\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3260\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2903val_aucs: 0.4020334091344199 0.8538884021114269\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2894\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2595val_aucs: 0.39950273231247657 0.8530335733977303\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2603\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2677val_aucs: 0.43981937136199606 0.8592951937255573\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2660\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2438val_aucs: 0.41229059438281823 0.8427969995512149\n",
      "2879/2879 [==============================] - 11s 4ms/sample - loss: 0.2467\n",
      "Test res 0.8768801159660342 0.5422800846910067 0.5165692007797271\n",
      "Repeat 4 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5293val_aucs: 0.4113894676365585 0.849942253753506\n",
      "2879/2879 [==============================] - 16s 6ms/sample - loss: 0.5285\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4657val_aucs: 0.44012046878021815 0.8602540834845736\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4649\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4258val_aucs: 0.4380281353360164 0.863553868998515\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4233\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4099val_aucs: 0.4467646039626081 0.8594910080844744\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4070\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3837val_aucs: 0.4090503044264103 0.8497153934994226\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3842\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3603val_aucs: 0.3821949895812846 0.8340001649892758\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3588\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3319val_aucs: 0.378203327843235 0.8275449595776274\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3298\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3251val_aucs: 0.3897607666043109 0.8179343342682726\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3252\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3003val_aucs: 0.3891868774419658 0.8285142715723478\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2988\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2555val_aucs: 0.3666362191009744 0.8056632568883022\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2582\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2583val_aucs: 0.3495471062305483 0.8103035802672826\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2576\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2447val_aucs: 0.3555572875078937 0.800445471044382\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2441\n",
      "Epoch 13/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2097val_aucs: 0.3703780298799525 0.8112935159214651\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2091\n",
      "Epoch 14/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2316val_aucs: 0.3309012285843847 0.7762332948358356\n",
      "2879/2879 [==============================] - 11s 4ms/sample - loss: 0.2309\n",
      "Test res 0.8686088594157608 0.5101814614325068 0.48830409356725146\n",
      "Repeat 5 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5346val_aucs: 0.5168359931810727 0.8754543094496365\n",
      "2879/2879 [==============================] - 17s 6ms/sample - loss: 0.5377\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4683val_aucs: 0.4686388748202986 0.8696131879543094\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4677\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4230val_aucs: 0.46859777916639195 0.8636638975424022\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4228\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3931val_aucs: 0.4667491066021915 0.8619764624437523\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3919\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3541val_aucs: 0.4103034087905255 0.8392609899619247\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3521\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3396val_aucs: 0.33631680109178813 0.828054690204223\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3436\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3187val_aucs: 0.36380320858724163 0.8359293873312565\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3161\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2808val_aucs: 0.35365218693201195 0.8226462443752163\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2838\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2668val_aucs: 0.37431936988772685 0.83389581169955\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2670\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2575val_aucs: 0.3334392846122632 0.8203746971270336\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2568\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2291val_aucs: 0.3238001866439747 0.8153340256143994\n",
      "2879/2879 [==============================] - 11s 4ms/sample - loss: 0.2313\n",
      "Test res 0.8775333434622387 0.535772437964947 0.5154142581888247\n",
      "Repeat 6 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5605val_aucs: 0.500129833434872 0.8528249288755657\n",
      "2879/2879 [==============================] - 17s 6ms/sample - loss: 0.5606\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4494val_aucs: 0.52382259479067 0.865235903996334\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4475\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4316val_aucs: 0.5204156604996338 0.8654459358829931\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4325\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3948val_aucs: 0.5050408906839775 0.8612643919576881\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3927\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3936val_aucs: 0.4838605138685712 0.8592404483226089\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3922\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3583val_aucs: 0.45867501021853896 0.8442327153304183\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3580\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3515val_aucs: 0.4573888831206846 0.8410822370305309\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3509\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3145val_aucs: 0.4849269661434357 0.8411395184541653\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3146\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3015val_aucs: 0.43828972720198867 0.8357932522482958\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3012\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2665val_aucs: 0.40332690369924346 0.8142745307696715\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2653\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2403val_aucs: 0.46380556463483785 0.8230958700093559\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2398\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2252val_aucs: 0.3903940074786459 0.8052431596433277\n",
      "2879/2879 [==============================] - 12s 4ms/sample - loss: 0.2251\n",
      "Test res 0.8692153524412152 0.5118414154802188 0.4917234664070107\n",
      "Repeat 7 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5484val_aucs: 0.4814474641095234 0.8513469358117921\n",
      "2879/2879 [==============================] - 18s 6ms/sample - loss: 0.5490\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4754val_aucs: 0.4952695612212241 0.8531077548649434\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4730\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4385val_aucs: 0.49786094565355427 0.8522545744989833\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4372\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4281val_aucs: 0.4911428883468973 0.8444125762416498\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4270\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3987val_aucs: 0.5288210612055098 0.8571921289573047\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3976\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3833val_aucs: 0.5122204095914764 0.8482791170490852\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3860\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3603val_aucs: 0.49847210523563257 0.8371333139703747\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3599\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3469val_aucs: 0.47620744796280007 0.8359533836770259\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3453\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3027val_aucs: 0.4759521756903301 0.8272218995062446\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3040\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2952val_aucs: 0.4538931494798267 0.8195432762126053\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2955\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2737val_aucs: 0.48621329519256073 0.8223569561428986\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2750\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2679val_aucs: 0.46556211760755806 0.8289101074644205\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2662\n",
      "Epoch 13/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2378val_aucs: 0.4702552953145943 0.8057199390066803\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2409\n",
      "Epoch 14/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2410val_aucs: 0.4426555779587848 0.8211770258495498\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2432\n",
      "Epoch 15/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2015val_aucs: 0.43697143742807565 0.8078075079872205\n",
      "2879/2879 [==============================] - 12s 4ms/sample - loss: 0.2009\n",
      "Test res 0.8781137694522149 0.5263467684266767 0.5092322643343051\n",
      "Repeat 8 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5841val_aucs: 0.44471818187696555 0.832947359776628\n",
      "2879/2879 [==============================] - 19s 6ms/sample - loss: 0.5823\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4730val_aucs: 0.48602528394633115 0.83846596041718\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4713\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4285val_aucs: 0.46963112805737733 0.8384988092305167\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4287\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4023val_aucs: 0.4558908812530095 0.834113492650078\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3994\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3898val_aucs: 0.4566829070757793 0.8376611644904328\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3913\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3627val_aucs: 0.4097128987676874 0.8146177219347951\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3612\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3397val_aucs: 0.4464028280167293 0.8297281760696396\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3396\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3276val_aucs: 0.4563403557184867 0.8291040486162436\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3299\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2928val_aucs: 0.4462914231438976 0.8206126303687279\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2931\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2649val_aucs: 0.43980106022527876 0.8259012893159234\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2633\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2500val_aucs: 0.4325447394137063 0.8126796419479346\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2507\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2211val_aucs: 0.4176787372623153 0.8048944731871562\n",
      "2879/2879 [==============================] - 12s 4ms/sample - loss: 0.2216\n",
      "Test res 0.8773547222048441 0.5382755940668476 0.5189873417721519\n",
      "Repeat 9 ld 10\n",
      "Num train: 2879 Num valid: 714\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_10ld.h5\n",
      "Train on 2879 samples\n",
      "Epoch 1/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.5127val_aucs: 0.505536614575622 0.8586247191011236\n",
      "2879/2879 [==============================] - 19s 7ms/sample - loss: 0.5146\n",
      "Epoch 2/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4364val_aucs: 0.50588350852239 0.8523685393258428\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4352\n",
      "Epoch 3/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3951val_aucs: 0.5014830954659388 0.8559820224719101\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3940\n",
      "Epoch 4/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.4048val_aucs: 0.5123729818927829 0.8553887640449438\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.4037\n",
      "Epoch 5/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3657val_aucs: 0.5021294269414249 0.8526022471910113\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3635\n",
      "Epoch 6/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.3330val_aucs: 0.48625000846661254 0.8387595505617977\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.3321\n",
      "Epoch 7/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2930val_aucs: 0.514521935736899 0.8538786516853932\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2951\n",
      "Epoch 8/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2804val_aucs: 0.4711899136116559 0.8162876404494381\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2809\n",
      "Epoch 9/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2697val_aucs: 0.491051249047634 0.8330786516853932\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2681\n",
      "Epoch 10/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2643val_aucs: 0.45180199862819426 0.8200089887640449\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2634\n",
      "Epoch 11/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2157val_aucs: 0.47192902617596244 0.8154067415730337\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2164\n",
      "Epoch 12/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2015val_aucs: 0.44792873830763863 0.806238202247191\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2020\n",
      "Epoch 13/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.2194val_aucs: 0.48795217054096474 0.8289977528089887\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.2196\n",
      "Epoch 14/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.1832val_aucs: 0.4455548880855845 0.8139505617977527\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.1831\n",
      "Epoch 15/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.1564val_aucs: 0.4877969621984747 0.8242337078651686\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.1584\n",
      "Epoch 16/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.1406val_aucs: 0.49770393432282367 0.8058966292134832\n",
      "2879/2879 [==============================] - 10s 3ms/sample - loss: 0.1395\n",
      "Epoch 17/1000\n",
      "2848/2879 [============================>.] - ETA: 0s - loss: 0.1625val_aucs: 0.4822539512032983 0.8202966292134831\n",
      "2879/2879 [==============================] - 12s 4ms/sample - loss: 0.1615\n",
      "Test res 0.8581656602308398 0.4918865761586767 0.4903660886319846\n",
      "gen_res {10: [(0.8734300631368426, 0.006136223098497274), (0.5248239434565196, 0.014754366403570052), (0.5043583696473467, 0.01070374615713679)]}\n",
      "Repeat 0 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5294val_aucs: 0.5323917861840857 0.8816431098645255\n",
      "5758/5758 [==============================] - 30s 5ms/sample - loss: 0.5311\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4419val_aucs: 0.5282466160671966 0.8845716695081487\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4421\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4334val_aucs: 0.5497622386005758 0.881116060789382\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4335\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4025val_aucs: 0.5267882885332692 0.8856303506938715\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4038\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3773val_aucs: 0.5243192570885656 0.8774496324405581\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3774\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3713val_aucs: 0.5173846389354586 0.8656070688738564\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3712\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3576val_aucs: 0.48818732964153505 0.8682606463913178\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3580\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3302val_aucs: 0.5152215571906419 0.8705246658967167\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3301\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3077val_aucs: 0.5242315561699009 0.8691657958899339\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3071\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2988val_aucs: 0.5378144325409385 0.8721745586536875\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2983\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2710val_aucs: 0.523465525818144 0.8590899924838218\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2706\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2586val_aucs: 0.519828439437138 0.8667207464848118\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2582\n",
      "Epoch 13/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2434val_aucs: 0.5014539718263588 0.866152450090744\n",
      "5758/5758 [==============================] - 22s 4ms/sample - loss: 0.2431\n",
      "Test res 0.880894004979052 0.5326610415038262 0.4980582524271845\n",
      "Repeat 1 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4978val_aucs: 0.5115945768611504 0.8818375502048972\n",
      "5758/5758 [==============================] - 30s 5ms/sample - loss: 0.4969\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4368val_aucs: 0.5416282463419707 0.8899162062427368\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4365\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4110val_aucs: 0.5003047082723802 0.8791004913453893\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4110\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4008val_aucs: 0.5182319574537181 0.8762156211135803\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4013\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3804val_aucs: 0.49323234831789653 0.8756141817366307\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3797\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3658val_aucs: 0.4871101759859014 0.8725967909641379\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3648\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3572val_aucs: 0.504202678531048 0.8718271524393972\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3573\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3344val_aucs: 0.4890410392229623 0.8690187363656751\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3336\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3146val_aucs: 0.4823785715362272 0.8598187526758956\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3145\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3025val_aucs: 0.45265145172200105 0.850888907011356\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3029\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2862val_aucs: 0.47378458748715724 0.853450121307264\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2872\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2790val_aucs: 0.40387412385331395 0.8378967970804706\n",
      "5758/5758 [==============================] - 22s 4ms/sample - loss: 0.2795\n",
      "Test res 0.8857331639224286 0.5590288536566508 0.5267249757045676\n",
      "Repeat 2 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4986val_aucs: 0.5083532284260925 0.866497732052809\n",
      "5758/5758 [==============================] - 31s 5ms/sample - loss: 0.4977\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4310val_aucs: 0.5445934448388408 0.8820363378354409\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4301\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4189val_aucs: 0.5342337711400535 0.8659098395965841\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4208\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4015val_aucs: 0.5197132976713738 0.8628842206623927\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4015\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3863val_aucs: 0.5191592302106144 0.8661277652484606\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3860\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3691val_aucs: 0.5332788347372618 0.8662899424777639\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3686\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3491val_aucs: 0.49303750708067423 0.8607809847198641\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3491\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3342val_aucs: 0.5162781093829334 0.8632845956972353\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3341\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3171val_aucs: 0.4997067279504537 0.8576235967868636\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3178\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3112val_aucs: 0.4640927967759566 0.8557991029572004\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3101\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2837val_aucs: 0.4800093285517705 0.842703291690951\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2836\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2710val_aucs: 0.47270072512585853 0.8357955553303094\n",
      "5758/5758 [==============================] - 23s 4ms/sample - loss: 0.2705\n",
      "Test res 0.8853438961926239 0.5498596294828736 0.5311890838206628\n",
      "Repeat 3 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5015val_aucs: 0.5395650364245843 0.8529057883178051\n",
      "5758/5758 [==============================] - 32s 6ms/sample - loss: 0.4996\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4427val_aucs: 0.5374795505093309 0.8565645812739577\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4430\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4168val_aucs: 0.562141533742227 0.8622451033452461\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4163\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4059val_aucs: 0.5417197154111456 0.8524987587835632\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4060\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3926val_aucs: 0.5241049740836898 0.8553569112273058\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3928\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3809val_aucs: 0.5303831237775111 0.851049554727581\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3797\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3787val_aucs: 0.5381319230835893 0.856658511166475\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3784\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3398val_aucs: 0.48647029887072046 0.8361907403017386\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3414\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3314val_aucs: 0.5273750963172836 0.8393933023513783\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3318\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3189val_aucs: 0.48085791701936637 0.8387000102875596\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3179\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3071val_aucs: 0.4907621256474886 0.8372731704917006\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3066\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2961val_aucs: 0.4902131603627737 0.8370003265181978\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2962\n",
      "Epoch 13/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2915val_aucs: 0.49270281627993373 0.8338648572489277\n",
      "5758/5758 [==============================] - 23s 4ms/sample - loss: 0.2914\n",
      "Test res 0.8841501500964728 0.5484327144056104 0.5243664717348928\n",
      "Repeat 4 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5016val_aucs: 0.44219596316142373 0.8452097505668934\n",
      "5758/5758 [==============================] - 33s 6ms/sample - loss: 0.5040\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4199val_aucs: 0.42320158504733385 0.8407171201814058\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4199\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4036val_aucs: 0.44669587111036413 0.8457908163265305\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4024\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3808val_aucs: 0.4530972363464449 0.8453798185941043\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3811\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3574val_aucs: 0.4451165093065077 0.8416855631141346\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3573\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3398val_aucs: 0.4263864119695124 0.8320625472411186\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3399\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3207val_aucs: 0.41575374059072884 0.8341175359032502\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3198\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3072val_aucs: 0.410492845468442 0.8269652305366592\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3063\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2835val_aucs: 0.4394727857788986 0.8308862433862434\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2832\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2520val_aucs: 0.40944110677687856 0.822855253212396\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2524\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2512val_aucs: 0.37414291267165745 0.8066562736205594\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2506\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2215val_aucs: 0.4005425551074691 0.8190806878306878\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2206\n",
      "Epoch 13/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2112val_aucs: 0.36241566301037925 0.8170375094482238\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2112\n",
      "Epoch 14/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2022val_aucs: 0.38656242932482887 0.8121693121693121\n",
      "5758/5758 [==============================] - 23s 4ms/sample - loss: 0.2026\n",
      "Test res 0.8736736844091323 0.5216401871091731 0.511175898931001\n",
      "Repeat 5 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5159val_aucs: 0.45658915307058423 0.859320862893912\n",
      "5758/5758 [==============================] - 34s 6ms/sample - loss: 0.5153\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4350val_aucs: 0.4938082899363557 0.8640282131661442\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4344\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4079val_aucs: 0.48102191250736576 0.8681838805477644\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4087\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3970val_aucs: 0.45927958041633077 0.861553374030688\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3969\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3834val_aucs: 0.4811205781359576 0.8636363636363636\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3838\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3618val_aucs: 0.43748568738303717 0.8437242204256724\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3607\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3445val_aucs: 0.46325081165526966 0.8539019963702359\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3444\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3285val_aucs: 0.42190332401965663 0.8420330803497772\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3282\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3026val_aucs: 0.40512057715680555 0.834206401583897\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3024\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2958val_aucs: 0.413414801692518 0.8411101715888467\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2958\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2890val_aucs: 0.42579078860472364 0.8506125226860254\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2891\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2651val_aucs: 0.3783464902208326 0.8361037782544135\n",
      "5758/5758 [==============================] - 23s 4ms/sample - loss: 0.2657\n",
      "Test res 0.8745084501881294 0.5244950329144168 0.5102239532619279\n",
      "Repeat 6 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5592val_aucs: 0.4442010069090482 0.8683121876201461\n",
      "5758/5758 [==============================] - 35s 6ms/sample - loss: 0.5598\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4734val_aucs: 0.4805052894136146 0.8733205177495835\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4734\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4544val_aucs: 0.4996128503885913 0.8731359733435857\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4533\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4276val_aucs: 0.4831693248211091 0.8684454696911443\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4271\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4148val_aucs: 0.4731165292965681 0.8604024093297449\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4153\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3891val_aucs: 0.46465211247417637 0.8642675893886967\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3905\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3687val_aucs: 0.43923938092244186 0.8507497116493656\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3685\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3705val_aucs: 0.4009607876298197 0.8439420735614507\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3697\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3490val_aucs: 0.4443075681510845 0.8582237600922723\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3494\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3306val_aucs: 0.4098118439622481 0.8427886710239652\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3297\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3073val_aucs: 0.4205569835515519 0.8417531718569781\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3072\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2883val_aucs: 0.39656114209881627 0.8281173907471485\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2893\n",
      "Epoch 13/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2769val_aucs: 0.41685104675983214 0.814004869921825\n",
      "5758/5758 [==============================] - 24s 4ms/sample - loss: 0.2768\n",
      "Test res 0.8831403627935519 0.5337874724127792 0.5038986354775828\n",
      "Repeat 7 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5453val_aucs: 0.4950653424322058 0.8630778750110493\n",
      "5758/5758 [==============================] - 36s 6ms/sample - loss: 0.5447\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4704val_aucs: 0.4851061870995647 0.8628313026001758\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4706\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4616val_aucs: 0.4954405586782227 0.8626591671435283\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4617\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4286val_aucs: 0.5250315727827787 0.8741364150232382\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4278\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4066val_aucs: 0.4950830662224769 0.8537081233978607\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4063\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3922val_aucs: 0.4976873184280643 0.8552433855787707\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3920\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3793val_aucs: 0.46322846343037327 0.8469715790404145\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3791\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3566val_aucs: 0.4732896067612836 0.8463900403355246\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3560\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3382val_aucs: 0.4754482370618659 0.8417702968638782\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3386\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3225val_aucs: 0.45681954117839524 0.8369830702452231\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3234\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2980val_aucs: 0.4430401863118931 0.8357129897137434\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2999\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2838val_aucs: 0.47621760769347843 0.8474228530754093\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2835\n",
      "Epoch 13/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2516val_aucs: 0.42807198820390646 0.8231447752236597\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2514\n",
      "Epoch 14/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2603val_aucs: 0.4313645888201296 0.8109487455047057\n",
      "5758/5758 [==============================] - 24s 4ms/sample - loss: 0.2596\n",
      "Test res 0.876516977336078 0.540509352877359 0.51364522417154\n",
      "Repeat 8 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5065val_aucs: 0.5073840659775531 0.8613789548049567\n",
      "5758/5758 [==============================] - 36s 6ms/sample - loss: 0.5068\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4489val_aucs: 0.5262424255594736 0.8744391193935808\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4517\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4203val_aucs: 0.5184469520983802 0.8690451186680159\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4198\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4102val_aucs: 0.5229041240447628 0.8707731082809845\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4125\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3924val_aucs: 0.5187976651930225 0.8653027323239074\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3922\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3876val_aucs: 0.5078233116559668 0.8641618773031905\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3873\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3702val_aucs: 0.5238972385014308 0.8589826819162546\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3696\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3449val_aucs: 0.4870591457889529 0.8474022874381837\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3437\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3239val_aucs: 0.5003727458349545 0.8546722547877723\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3250\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3114val_aucs: 0.500237774838112 0.8544097149294483\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3109\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2951val_aucs: 0.4857230182243325 0.8438603860767954\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2950\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2793val_aucs: 0.49271537596115084 0.8508678135680598\n",
      "5758/5758 [==============================] - 24s 4ms/sample - loss: 0.2787\n",
      "Test res 0.8815754096993204 0.5485121419588912 0.5165692007797271\n",
      "Repeat 9 ld 20\n",
      "Num train: 5758 Num valid: 1428\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_20ld.h5\n",
      "Train on 5758 samples\n",
      "Epoch 1/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.5020val_aucs: 0.5439968215022807 0.8623486467236468\n",
      "5758/5758 [==============================] - 37s 6ms/sample - loss: 0.5021\n",
      "Epoch 2/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4346val_aucs: 0.5616623411339768 0.8628205128205128\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4345\n",
      "Epoch 3/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.4179val_aucs: 0.5381380712263892 0.8565705128205129\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.4185\n",
      "Epoch 4/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3984val_aucs: 0.5441493753304819 0.868184650997151\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3982\n",
      "Epoch 5/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3805val_aucs: 0.5386748367244981 0.8666978276353277\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3807\n",
      "Epoch 6/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3756val_aucs: 0.5086589776424564 0.8610131766381767\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3756\n",
      "Epoch 7/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3524val_aucs: 0.5262700853535255 0.8665375712250712\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3523\n",
      "Epoch 8/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3400val_aucs: 0.5015091607684224 0.8554531695156695\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3394\n",
      "Epoch 9/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.3150val_aucs: 0.4956781114340698 0.8453169515669515\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.3176\n",
      "Epoch 10/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2928val_aucs: 0.4939037515304541 0.8428997507122508\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2929\n",
      "Epoch 11/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2945val_aucs: 0.4882146315497243 0.8363871082621083\n",
      "5758/5758 [==============================] - 20s 3ms/sample - loss: 0.2939\n",
      "Epoch 12/1000\n",
      "5728/5758 [============================>.] - ETA: 0s - loss: 0.2789val_aucs: 0.4773737778760935 0.8442752849002849\n",
      "5758/5758 [==============================] - 24s 4ms/sample - loss: 0.2786\n",
      "Test res 0.8800131874373771 0.5487952704317419 0.5272904483430799\n",
      "gen_res {10: [(0.8734300631368426, 0.006136223098497274), (0.5248239434565196, 0.014754366403570052), (0.5043583696473467, 0.01070374615713679)], 20: [(0.8805549287054166, 0.004135513636745678), (0.5407721696753323, 0.011595141675669765), (0.5163142144652165, 0.010358914479794616)]}\n",
      "Repeat 0 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.5094val_aucs: 0.49059668347419116 0.881154016980133\n",
      "8637/8637 [==============================] - 49s 6ms/sample - loss: 0.5089\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4439val_aucs: 0.5109378376793512 0.8781004971044943\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4440\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4328val_aucs: 0.5116936499084647 0.8816366012401988\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4326\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4085val_aucs: 0.5239791520065169 0.8808315823638942\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.4089\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3968val_aucs: 0.48550494986049275 0.8744597618681564\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3965\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3925val_aucs: 0.4725145673346204 0.8699777071695792\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3921\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3636val_aucs: 0.44831028673298196 0.8615153999897504\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3630\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3623val_aucs: 0.4700773693905765 0.8646372503800885\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3625\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3462val_aucs: 0.4211333353677966 0.8501042040349169\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3461\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3339val_aucs: 0.4706177699225307 0.8661362510463109\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3340\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3225val_aucs: 0.4472633261021807 0.858173610755223\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3221\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3155val_aucs: 0.4580560303447685 0.8596299048497583\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3153\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2977val_aucs: 0.4011970943321688 0.8429038760484463\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.2988\n",
      "Epoch 14/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2953val_aucs: 0.454536845032522 0.8637404123746563\n",
      "8637/8637 [==============================] - 35s 4ms/sample - loss: 0.2950\n",
      "Test res 0.8911057040631805 0.5714404530245791 0.5341130604288499\n",
      "Repeat 1 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4875val_aucs: 0.5215612205188153 0.8687730210194313\n",
      "8637/8637 [==============================] - 49s 6ms/sample - loss: 0.4876\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4239val_aucs: 0.522692619051686 0.8743223273561392\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4235\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4147val_aucs: 0.5313966658650959 0.8711969827300998\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4148\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4005val_aucs: 0.5150574096096874 0.8650146672842365\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4005\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3812val_aucs: 0.522272803346204 0.8734422903019475\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3823\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3656val_aucs: 0.5316885122293337 0.8713822536888772\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3666\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3570val_aucs: 0.4888939561482204 0.8613511546350825\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3568\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3488val_aucs: 0.480195497798024 0.8553033811949977\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3483\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3297val_aucs: 0.5083498307384413 0.8624914532742232\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3290\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3190val_aucs: 0.48940783490651046 0.8609541454377027\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3193\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3044val_aucs: 0.4709092920363994 0.8429773484196829\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3037\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2850val_aucs: 0.4566226531789223 0.8561768014292332\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2845\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2699val_aucs: 0.44087364547631863 0.849900747700655\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2698\n",
      "Epoch 14/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2682val_aucs: 0.4722977424526002 0.8667648161626855\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2700\n",
      "Epoch 15/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2600val_aucs: 0.4796528957497372 0.855731268885507\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2601\n",
      "Epoch 16/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2548val_aucs: 0.4628907749158538 0.8518791768675974\n",
      "8637/8637 [==============================] - 35s 4ms/sample - loss: 0.2549\n",
      "Test res 0.8802691409000227 0.5486099112648977 0.5350877192982456\n",
      "Repeat 2 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4718val_aucs: 0.515884333498293 0.873962765022327\n",
      "8637/8637 [==============================] - 50s 6ms/sample - loss: 0.4714\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4273val_aucs: 0.5378514112340105 0.8769140501388841\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4269\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4026val_aucs: 0.5320149093318343 0.8743033824408424\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4032\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3929val_aucs: 0.5429527788805577 0.8756878274322282\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3934\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3876val_aucs: 0.5359096542048646 0.8760482226363349\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3881\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3699val_aucs: 0.542368117473124 0.8745604936535283\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3700\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3567val_aucs: 0.5386521463236706 0.8754526915368658\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3566\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3450val_aucs: 0.5121572981030434 0.8670449351288633\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3450\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3306val_aucs: 0.5191359921950839 0.8675481698955733\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3303\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3136val_aucs: 0.47615326442689276 0.8567604866214267\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3139\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3126val_aucs: 0.4732591381572019 0.8642123167258536\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3127\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2973val_aucs: 0.4623444926696762 0.8506557434689357\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2968\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2798val_aucs: 0.5025492908669387 0.8652792183819135\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2798\n",
      "Epoch 14/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2659val_aucs: 0.43882558716109 0.8400790232410956\n",
      "8637/8637 [==============================] - 36s 4ms/sample - loss: 0.2656\n",
      "Test res 0.8859451758109826 0.5568664253341236 0.5282651072124757\n",
      "Repeat 3 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4923val_aucs: 0.49947336985720175 0.8646765567750299\n",
      "8637/8637 [==============================] - 51s 6ms/sample - loss: 0.4921\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4329val_aucs: 0.5373147786306078 0.8729132190927165\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4323\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4107val_aucs: 0.5215700038669755 0.8675798135079594\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4101\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4021val_aucs: 0.538401810090586 0.8736781115700929\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4020\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3856val_aucs: 0.5224174467750257 0.8682425813766398\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3858\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3702val_aucs: 0.5225290197793623 0.8625173507625999\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3694\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3547val_aucs: 0.54323196881354 0.8670525170589779\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3552\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3469val_aucs: 0.5222654488943732 0.8641659337315499\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3470\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3307val_aucs: 0.49582678459065865 0.8591659962568203\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3307\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3160val_aucs: 0.4881544933602829 0.8500185491636203\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3156\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2987val_aucs: 0.5006458001410656 0.8555707931955832\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2990\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2895val_aucs: 0.5009516512879131 0.8567671100403078\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2889\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2777val_aucs: 0.4757184946470153 0.8500769060828751\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2779\n",
      "Epoch 14/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2744val_aucs: 0.4669189600973915 0.836112928975461\n",
      "8637/8637 [==============================] - 35s 4ms/sample - loss: 0.2737\n",
      "Test res 0.8863545523967273 0.5658591006659901 0.5326192794547225\n",
      "Repeat 4 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4904val_aucs: 0.4954550205260368 0.8749739884265101\n",
      "8637/8637 [==============================] - 50s 6ms/sample - loss: 0.4903\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4363val_aucs: 0.5353561886795677 0.8815643249609272\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4354\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4127val_aucs: 0.5025162749013067 0.8766830041485693\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4116\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4174val_aucs: 0.5374154789205177 0.8818963876012237\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4172\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3950val_aucs: 0.5267460953142477 0.881590889972151\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3953\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3824val_aucs: 0.5227213314629027 0.8756602512164561\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3820\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3667val_aucs: 0.5189313519493879 0.8697738874794674\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3662\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3569val_aucs: 0.5108852086649408 0.8725455036504755\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3569\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3406val_aucs: 0.4749696188575451 0.8557719128136332\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3415\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3275val_aucs: 0.4842312270259312 0.8589198666436437\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3275\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3051val_aucs: 0.478669747087912 0.8621940042769668\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3052\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2935val_aucs: 0.48535821710426874 0.8579170374699484\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2943\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2795val_aucs: 0.45697150832626016 0.8438132302610898\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.2790\n",
      "Epoch 14/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2800val_aucs: 0.4523931911161856 0.8445260580622596\n",
      "8637/8637 [==============================] - 35s 4ms/sample - loss: 0.2798\n",
      "Test res 0.8867086916930068 0.5550390995009146 0.5262645914396887\n",
      "Repeat 5 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4962val_aucs: 0.5343927330056096 0.8822300649410812\n",
      "8637/8637 [==============================] - 50s 6ms/sample - loss: 0.4960\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4378val_aucs: 0.5427790996624952 0.8830091894578135\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4375\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4222val_aucs: 0.548937163057891 0.8799369119715641\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4221\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4067val_aucs: 0.513707958424887 0.8700399248671277\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4062\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3996val_aucs: 0.5452802220096616 0.8723646639657017\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3990\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3806val_aucs: 0.5347445165898026 0.8740934780960723\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3820\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3690val_aucs: 0.5320246726752135 0.868071056155926\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3702\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3559val_aucs: 0.524480327482523 0.870968557061395\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3566\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3453val_aucs: 0.5144121039977326 0.8627730094421469\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3450\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3273val_aucs: 0.5106404467675071 0.8536677813060655\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3269\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3181val_aucs: 0.5225594381560986 0.8560556926627528\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3187\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3055val_aucs: 0.4869855779628582 0.8472557971075528\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3057\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2941val_aucs: 0.5239813959640608 0.8509324225297542\n",
      "8637/8637 [==============================] - 35s 4ms/sample - loss: 0.2937\n",
      "Test res 0.8834253623814446 0.5473887273954412 0.5311890838206628\n",
      "Repeat 6 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.5180val_aucs: 0.5222283764257197 0.8755733232922477\n",
      "8637/8637 [==============================] - 52s 6ms/sample - loss: 0.5183\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4627val_aucs: 0.5502113523287666 0.8814297330967269\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.4620\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4459val_aucs: 0.5524852002883449 0.8814016928309052\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.4463\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4238val_aucs: 0.5711942589935332 0.8871659703333987\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.4247\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4137val_aucs: 0.5405918514023695 0.8745318277045837\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.4152\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3979val_aucs: 0.5616105684647084 0.8820225844312433\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3991\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3857val_aucs: 0.5507617879109659 0.8793086872749268\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3853\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3791val_aucs: 0.5601908939709319 0.8791684859458181\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3797\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3597val_aucs: 0.507831421599797 0.8621099899455618\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3605\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3357val_aucs: 0.5065994656715869 0.8590435865903437\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3357\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3213val_aucs: 0.5101540978759177 0.8545190894123963\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3225\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3178val_aucs: 0.47304817532895643 0.848564538677541\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3181\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3022val_aucs: 0.4706628395356721 0.8499685548447571\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3023\n",
      "Epoch 14/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2944val_aucs: 0.45716180041894344 0.8417086936841304\n",
      "8637/8637 [==============================] - 36s 4ms/sample - loss: 0.2953\n",
      "Test res 0.8870402395585386 0.573293651901445 0.5433300876338851\n",
      "Repeat 7 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.5078val_aucs: 0.5463406530294758 0.8709833699155642\n",
      "8637/8637 [==============================] - 52s 6ms/sample - loss: 0.5072\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4531val_aucs: 0.5188518186770861 0.8712068181368768\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4535\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4402val_aucs: 0.5120857828614147 0.8700935318660893\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4402\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4126val_aucs: 0.5059679891273098 0.8695625951632358\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4133\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4061val_aucs: 0.4832573598340067 0.8579264796029346\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4052\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3944val_aucs: 0.5014168447297407 0.8648098712700955\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3939\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3744val_aucs: 0.4768453278151411 0.8429949971327441\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3750\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3618val_aucs: 0.49545973247200864 0.8607620968539281\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3619\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3459val_aucs: 0.4928195278717639 0.8593274801763857\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3453\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3388val_aucs: 0.43784691373208795 0.8459156433529098\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3387\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3211val_aucs: 0.4398462884259256 0.8367898598010717\n",
      "8637/8637 [==============================] - 36s 4ms/sample - loss: 0.3208\n",
      "Test res 0.8798602608292395 0.5496054055529453 0.5253411306042886\n",
      "Repeat 8 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.5143val_aucs: 0.5180168671200723 0.867389369792088\n",
      "8637/8637 [==============================] - 52s 6ms/sample - loss: 0.5135\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4476val_aucs: 0.5121557946159009 0.8618416794757706\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4486\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4303val_aucs: 0.5371978294522939 0.8737642585551331\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4313\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4077val_aucs: 0.5206389083311871 0.8662163255400048\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4079\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4017val_aucs: 0.5353983005768516 0.8728055982525686\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.4009\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3829val_aucs: 0.535097720340474 0.8673590324407411\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3824\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3664val_aucs: 0.5119631263446447 0.8518141736105493\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3673\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3547val_aucs: 0.49663945238733886 0.8602540247552786\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3543\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3425val_aucs: 0.5022568514426494 0.8623574144486692\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3425\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3248val_aucs: 0.47435903791231565 0.8408502548337513\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3245\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3126val_aucs: 0.4802309562240394 0.8417259930426341\n",
      "8637/8637 [==============================] - 30s 3ms/sample - loss: 0.3123\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2952val_aucs: 0.44440066170444303 0.8408967721058167\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.2955\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2755val_aucs: 0.46740119085207077 0.8350234608850418\n",
      "8637/8637 [==============================] - 36s 4ms/sample - loss: 0.2758\n",
      "Test res 0.8830470800451631 0.5449932919149679 0.5141187925998053\n",
      "Repeat 9 ld 30\n",
      "Num train: 8637 Num valid: 2143\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_30ld.h5\n",
      "Train on 8637 samples\n",
      "Epoch 1/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.5006val_aucs: 0.534773913925131 0.8665638409365051\n",
      "8637/8637 [==============================] - 53s 6ms/sample - loss: 0.5010\n",
      "Epoch 2/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4436val_aucs: 0.5600191059716779 0.8719859207846394\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.4436\n",
      "Epoch 3/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4300val_aucs: 0.5562202418705616 0.8760594016333472\n",
      "8637/8637 [==============================] - 31s 4ms/sample - loss: 0.4293\n",
      "Epoch 4/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.4130val_aucs: 0.5287709126488743 0.8655929287536335\n",
      "8637/8637 [==============================] - 31s 4ms/sample - loss: 0.4123\n",
      "Epoch 5/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3875val_aucs: 0.5277392515393702 0.870508789622511\n",
      "8637/8637 [==============================] - 31s 4ms/sample - loss: 0.3887\n",
      "Epoch 6/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3704val_aucs: 0.5259595037883678 0.867781930355342\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3700\n",
      "Epoch 7/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3599val_aucs: 0.5191405698624351 0.8566273160507011\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3604\n",
      "Epoch 8/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3484val_aucs: 0.5075663579562771 0.8625991180716222\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3487\n",
      "Epoch 9/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3362val_aucs: 0.4845605675715183 0.8476992742876353\n",
      "8637/8637 [==============================] - 31s 4ms/sample - loss: 0.3360\n",
      "Epoch 10/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3205val_aucs: 0.47297409011396135 0.8468905103715568\n",
      "8637/8637 [==============================] - 31s 4ms/sample - loss: 0.3204\n",
      "Epoch 11/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.3056val_aucs: 0.48413585647640234 0.8442961381028653\n",
      "8637/8637 [==============================] - 30s 4ms/sample - loss: 0.3051\n",
      "Epoch 12/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2831val_aucs: 0.4164113596796793 0.8222054141701766\n",
      "8637/8637 [==============================] - 31s 4ms/sample - loss: 0.2836\n",
      "Epoch 13/1000\n",
      "8608/8637 [============================>.] - ETA: 0s - loss: 0.2709val_aucs: 0.4632190700369254 0.8374957979869885\n",
      "8637/8637 [==============================] - 37s 4ms/sample - loss: 0.2704\n",
      "Test res 0.8898156340645013 0.5737610214842985 0.5428849902534113\n",
      "gen_res {10: [(0.8734300631368426, 0.006136223098497274), (0.5248239434565196, 0.014754366403570052), (0.5043583696473467, 0.01070374615713679)], 20: [(0.8805549287054166, 0.004135513636745678), (0.5407721696753323, 0.011595141675669765), (0.5163142144652165, 0.010358914479794616)], 30: [(0.8853571841742806, 0.0035227240740787163), (0.5586857088039603, 0.010818887784918586), (0.5313213842746036, 0.008174419170316733)]}\n",
      "Repeat 0 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4894val_aucs: 0.5555525269097726 0.8796638655462184\n",
      "11516/11516 [==============================] - 64s 6ms/sample - loss: 0.4892\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4500val_aucs: 0.5639334406075873 0.8839002801120448\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.4499\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4324val_aucs: 0.553714249714986 0.8837512605042015\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.4330\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4199val_aucs: 0.5705902605653663 0.8880649859943979\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.4199\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4111val_aucs: 0.5573383971444232 0.8843450980392157\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.4107\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3970val_aucs: 0.5456478644704004 0.8748504201680672\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3975\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3868val_aucs: 0.5495804916465407 0.8801075630252102\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3868\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3712val_aucs: 0.529893369703575 0.873789355742297\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3711\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3679val_aucs: 0.5374400317939172 0.8805294117647059\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3678\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3521val_aucs: 0.5174017859602642 0.8709613445378152\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3528\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3337val_aucs: 0.5159387457833889 0.8673579831932772\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3346\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3350val_aucs: 0.48424234346275113 0.8648829131652661\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3349\n",
      "Epoch 13/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3167val_aucs: 0.5414112636644366 0.8792347338935574\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3160\n",
      "Epoch 14/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3045val_aucs: 0.49243896159218326 0.8676918767507003\n",
      "11516/11516 [==============================] - 46s 4ms/sample - loss: 0.3047\n",
      "Test res 0.891723244546528 0.5697687655215127 0.530214424951267\n",
      "Repeat 1 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4802val_aucs: 0.5512709753587843 0.8756972508553018\n",
      "11516/11516 [==============================] - 65s 6ms/sample - loss: 0.4797\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4417val_aucs: 0.5648818165122715 0.8819273906929882\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4425\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4251val_aucs: 0.567000595159715 0.8801525703340383\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4249\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4108val_aucs: 0.5386835794802629 0.8824455540916086\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4129\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4046val_aucs: 0.5650621705525313 0.8782437803784365\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4051\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3950val_aucs: 0.5741869486356547 0.8760316212535568\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3945\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3828val_aucs: 0.5418545353354581 0.8698601623135773\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3825\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3724val_aucs: 0.5628068838676344 0.8757271448975298\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3729\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3533val_aucs: 0.5263245382345689 0.8697029417951926\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3534\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3467val_aucs: 0.5269486839228446 0.8640740043623159\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3467\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3404val_aucs: 0.5373834156327115 0.8680620910328944\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3407\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3223val_aucs: 0.5194480202071801 0.8654768099735383\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3222\n",
      "Epoch 13/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3153val_aucs: 0.5084352325989474 0.8529888506294357\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3154\n",
      "Epoch 14/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2953val_aucs: 0.5024163041569197 0.8517643020848327\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.2958\n",
      "Epoch 15/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2910val_aucs: 0.48409212491385134 0.851825197356038\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.2909\n",
      "Epoch 16/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2837val_aucs: 0.5017330806243738 0.8574209191864393\n",
      "11516/11516 [==============================] - 47s 4ms/sample - loss: 0.2835\n",
      "Test res 0.8880647361171933 0.5695923846049878 0.5341130604288499\n",
      "Repeat 2 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4987val_aucs: 0.5308463997576727 0.8805707985024873\n",
      "11516/11516 [==============================] - 65s 6ms/sample - loss: 0.4988\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4396val_aucs: 0.5324724232359523 0.878503093398729\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4398\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4277val_aucs: 0.543845925199079 0.8793189860552288\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4276\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4161val_aucs: 0.5279159840996258 0.8763561301511966\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.4164\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4045val_aucs: 0.525867124164685 0.8721344684342787\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4054\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3924val_aucs: 0.5505231433699314 0.8801045741273444\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3935\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3784val_aucs: 0.4946472972262184 0.8643753292709648\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3788\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3682val_aucs: 0.5177631853605876 0.8694769894959649\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3682\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3609val_aucs: 0.49320123545998534 0.8669395633342503\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3604\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3454val_aucs: 0.4975975890195814 0.8638834625551893\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3459\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3307val_aucs: 0.5062247221469726 0.8710668146152017\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3305\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3167val_aucs: 0.4822208236792446 0.8668381595326567\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3169\n",
      "Epoch 13/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3028val_aucs: 0.48972434616156973 0.8682484882674636\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3026\n",
      "Epoch 14/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2949val_aucs: 0.45927967446310425 0.8590265701271395\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.2952\n",
      "Epoch 15/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2787val_aucs: 0.4385106544001284 0.8478231983924585\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.2791\n",
      "Epoch 16/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2733val_aucs: 0.4560838451481365 0.854499531444503\n",
      "11516/11516 [==============================] - 47s 4ms/sample - loss: 0.2732\n",
      "Test res 0.8903362300016185 0.5708399486937248 0.5358166189111748\n",
      "Repeat 3 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4938val_aucs: 0.4998185924107342 0.8752248002594416\n",
      "11516/11516 [==============================] - 66s 6ms/sample - loss: 0.4933\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4357val_aucs: 0.5164855341667481 0.8766288805684129\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4356\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4265val_aucs: 0.5221298575099181 0.8809590495002801\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4261\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4113val_aucs: 0.5464623516806233 0.886536061401181\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4109\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3958val_aucs: 0.5441775145602066 0.8875568756940554\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3969\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3897val_aucs: 0.516976812705567 0.8785869768173197\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3903\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3774val_aucs: 0.5374729314046636 0.8805377025659169\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3768\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3675val_aucs: 0.518639795295761 0.8783658618080329\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3670\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3614val_aucs: 0.5080202440493551 0.873683137278025\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3615\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3468val_aucs: 0.511815217553117 0.8754053775170259\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.3465\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3330val_aucs: 0.5064825522295883 0.8738563440352999\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3333\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3213val_aucs: 0.4454121912425914 0.8511244926638954\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3211\n",
      "Epoch 13/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3078val_aucs: 0.4850651839221264 0.863549927769097\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3080\n",
      "Epoch 14/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2942val_aucs: 0.5025494100635692 0.8714474188507915\n",
      "11516/11516 [==============================] - 47s 4ms/sample - loss: 0.2938\n",
      "Test res 0.8901055988020087 0.576244148380972 0.5350194552529183\n",
      "Repeat 4 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4896val_aucs: 0.4814342705003173 0.8678899240347101\n",
      "11516/11516 [==============================] - 66s 6ms/sample - loss: 0.4894\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4395val_aucs: 0.4896560214721837 0.8711833092563657\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4394\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4221val_aucs: 0.5102736578235045 0.8736647110272513\n",
      "11516/11516 [==============================] - 40s 3ms/sample - loss: 0.4224\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4133val_aucs: 0.506580240767439 0.8764372475603645\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4130\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4022val_aucs: 0.5089771954330569 0.8777737649498314\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.4018\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3866val_aucs: 0.47207694506837417 0.8691035997523511\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3863\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3762val_aucs: 0.48699941127758134 0.8662377035486502\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3755\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3713val_aucs: 0.4815960102606463 0.8677253161944632\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3711\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3519val_aucs: 0.42397887267297274 0.8614001985121416\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3521\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3494val_aucs: 0.45051079058621213 0.8604223788044065\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3494\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3320val_aucs: 0.42563218520117935 0.8488310386509037\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3320\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3240val_aucs: 0.4419841194905006 0.8510765844118832\n",
      "11516/11516 [==============================] - 40s 4ms/sample - loss: 0.3239\n",
      "Epoch 13/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3139val_aucs: 0.4295711181995042 0.8482610532936309\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3140\n",
      "Epoch 14/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3025val_aucs: 0.4451557017009966 0.8481799777902257\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3030\n",
      "Epoch 15/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2891val_aucs: 0.40989365704087766 0.8444910423852905\n",
      "11516/11516 [==============================] - 47s 4ms/sample - loss: 0.2890\n",
      "Test res 0.8893892518413257 0.5632671349362872 0.5370370370370371\n",
      "Repeat 5 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4781val_aucs: 0.48716331350909403 0.8704148833564409\n",
      "11516/11516 [==============================] - 68s 6ms/sample - loss: 0.4779\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4348val_aucs: 0.4902798363780755 0.8709975988104567\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4356\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4188val_aucs: 0.46198640588107254 0.8623774288182932\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4183\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4043val_aucs: 0.5031921732875997 0.8731840376152873\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4039\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3947val_aucs: 0.4549926999037226 0.8647735949524785\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3951\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3810val_aucs: 0.4770817093064123 0.8652106315429903\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3811\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3688val_aucs: 0.46323849775810494 0.8572146201298048\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3685\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3592val_aucs: 0.45645070276123023 0.8556397813812366\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3593\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3510val_aucs: 0.4530742279450426 0.857583840697651\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3514\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3357val_aucs: 0.46187089202472187 0.858759318423855\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3357\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3198val_aucs: 0.4583713906010989 0.8596019952980891\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3197\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3135val_aucs: 0.4336489655380106 0.854555980870858\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3142\n",
      "Epoch 13/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2996val_aucs: 0.43637020994089104 0.853877820644202\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.2992\n",
      "Epoch 14/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2886val_aucs: 0.4123519656176721 0.8429117186087166\n",
      "11516/11516 [==============================] - 48s 4ms/sample - loss: 0.2884\n",
      "Test res 0.885085832541392 0.5433445361265095 0.52046783625731\n",
      "Repeat 6 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4842val_aucs: 0.5154414101968505 0.8731844691718808\n",
      "11516/11516 [==============================] - 68s 6ms/sample - loss: 0.4844\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4226val_aucs: 0.5148765441395227 0.8700548249722129\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4231\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4118val_aucs: 0.5127521758849525 0.8697026463352191\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4121\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4014val_aucs: 0.5378780932633659 0.8796947785146053\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4011\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3892val_aucs: 0.511983746993597 0.8726774318434428\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3889\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3764val_aucs: 0.5161512042111641 0.8753649794562463\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3765\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3616val_aucs: 0.48393056533263307 0.8692617986087694\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3620\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3528val_aucs: 0.506343487187258 0.8661571316173989\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3522\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3490val_aucs: 0.4922172999539228 0.8642850898555067\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3493\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3326val_aucs: 0.5086322383259594 0.8609081712936946\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3324\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3174val_aucs: 0.4812962921229773 0.8506163126147391\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3171\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3103val_aucs: 0.4657091402394662 0.8504452187379016\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3105\n",
      "Epoch 13/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2997val_aucs: 0.48277612614238297 0.8560076430257391\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.2995\n",
      "Epoch 14/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2910val_aucs: 0.474056147219891 0.8522186005270191\n",
      "11516/11516 [==============================] - 48s 4ms/sample - loss: 0.2908\n",
      "Test res 0.8876621245477989 0.5642062144430231 0.5292397660818714\n",
      "Repeat 7 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4813val_aucs: 0.5358994084798447 0.8779399501677818\n",
      "11516/11516 [==============================] - 69s 6ms/sample - loss: 0.4809\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4258val_aucs: 0.5266155850036696 0.8792359897120582\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4259\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4099val_aucs: 0.5288945451438557 0.8760304267888361\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4096\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3962val_aucs: 0.521112885933909 0.881551781300862\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3964\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3825val_aucs: 0.49847919626111115 0.8690510277894992\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3826\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3783val_aucs: 0.520176009624693 0.8738822914782888\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3784\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3575val_aucs: 0.5195608692304724 0.8711131372194425\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3574\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3452val_aucs: 0.49745906501033405 0.8664677396668474\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3450\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3425val_aucs: 0.4854394870492657 0.8618261096710671\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3437\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3209val_aucs: 0.4777793554735454 0.856608043482629\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3210\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3074val_aucs: 0.45670025945104925 0.8496870416139208\n",
      "11516/11516 [==============================] - 48s 4ms/sample - loss: 0.3075\n",
      "Test res 0.8856475150915722 0.5541648813094309 0.5233918128654971\n",
      "Repeat 8 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4656val_aucs: 0.5338905148324757 0.8748666841507124\n",
      "11516/11516 [==============================] - 70s 6ms/sample - loss: 0.4657\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4296val_aucs: 0.5264360803078784 0.8819314875176402\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4295\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4083val_aucs: 0.5103504323217535 0.8752663194834712\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4085\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3917val_aucs: 0.505390841940317 0.8685449527306334\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3915\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3864val_aucs: 0.5162714058956941 0.8783435115457144\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3861\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3625val_aucs: 0.4937863468773557 0.8719668302673809\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3620\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3541val_aucs: 0.5024780129408775 0.8681328287937257\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3549\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3390val_aucs: 0.47355743225701186 0.8629550535136686\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3392\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3296val_aucs: 0.4772362026767991 0.8644337042448766\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3296\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3116val_aucs: 0.4847051917450226 0.8660797022716771\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3120\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.2982val_aucs: 0.47124521596365865 0.8584666491826209\n",
      "11516/11516 [==============================] - 48s 4ms/sample - loss: 0.2984\n",
      "Test res 0.8839515441118788 0.5634658414943582 0.5311890838206628\n",
      "Repeat 9 ld 40\n",
      "Num train: 11516 Num valid: 2857\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_40ld.h5\n",
      "Train on 11516 samples\n",
      "Epoch 1/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4880val_aucs: 0.5178300428497467 0.8673099364445601\n",
      "11516/11516 [==============================] - 70s 6ms/sample - loss: 0.4880\n",
      "Epoch 2/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4397val_aucs: 0.5351438025537284 0.8770735082795668\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4397\n",
      "Epoch 3/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4185val_aucs: 0.5209602584672404 0.8739945895900241\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4179\n",
      "Epoch 4/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.4097val_aucs: 0.5011502120449467 0.866710788994011\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.4098\n",
      "Epoch 5/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3950val_aucs: 0.5267436025721148 0.8663431029328087\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3950\n",
      "Epoch 6/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3805val_aucs: 0.5115991556605658 0.8629808851469298\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3800\n",
      "Epoch 7/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3662val_aucs: 0.5085289468252641 0.86288082631716\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3671\n",
      "Epoch 8/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3491val_aucs: 0.47913104877414153 0.8522179305422948\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3490\n",
      "Epoch 9/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3465val_aucs: 0.49402325299517147 0.8596295171136764\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3464\n",
      "Epoch 10/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3275val_aucs: 0.455372762354178 0.8430920830560619\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3279\n",
      "Epoch 11/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3188val_aucs: 0.47006384275746244 0.8589001726316198\n",
      "11516/11516 [==============================] - 41s 4ms/sample - loss: 0.3184\n",
      "Epoch 12/1000\n",
      "11488/11516 [============================>.] - ETA: 0s - loss: 0.3052val_aucs: 0.45546139190179297 0.8475609756097561\n",
      "11516/11516 [==============================] - 49s 4ms/sample - loss: 0.3057\n",
      "Test res 0.8836577313834197 0.5446709393272282 0.521865889212828\n",
      "gen_res {10: [(0.8734300631368426, 0.006136223098497274), (0.5248239434565196, 0.014754366403570052), (0.5043583696473467, 0.01070374615713679)], 20: [(0.8805549287054166, 0.004135513636745678), (0.5407721696753323, 0.011595141675669765), (0.5163142144652165, 0.010358914479794616)], 30: [(0.8853571841742806, 0.0035227240740787163), (0.5586857088039603, 0.010818887784918586), (0.5313213842746036, 0.008174419170316733)], 40: [(0.8875623808984734, 0.0027047954157572197), (0.5619564794838035, 0.01057049196019184), (0.5298354984819416, 0.0057264861003417005)]}\n",
      "Repeat 0 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.5017val_aucs: 0.5063564152061687 0.8699785676916126\n",
      "14395/14395 [==============================] - 81s 6ms/sample - loss: 0.5014\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4539val_aucs: 0.5353934069166738 0.8745496539915817\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4538\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4418val_aucs: 0.5268100994327032 0.8735902535017004\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4419\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4209val_aucs: 0.5251498264160843 0.8741539291574517\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4214\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4095val_aucs: 0.5578382302975037 0.8791749898932251\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4097\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3970val_aucs: 0.5338955950359918 0.87495318185061\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3968\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3855val_aucs: 0.5470160462049721 0.880375169437112\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3855\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3744val_aucs: 0.5288730071206743 0.8677097453092674\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3743\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3685val_aucs: 0.5186123074501359 0.8600003567097096\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3685\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3530val_aucs: 0.5200392120440045 0.8590922332405888\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3526\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3417val_aucs: 0.5048290624341957 0.8518874402511237\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3415\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3298val_aucs: 0.5128495751454141 0.8541079581936221\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3296\n",
      "Epoch 13/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3249val_aucs: 0.5061957960713164 0.8564614991320063\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3248\n",
      "Epoch 14/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3141val_aucs: 0.48774537193930545 0.8566666072150485\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3141\n",
      "Epoch 15/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3036val_aucs: 0.48198049250843444 0.8543219840194051\n",
      "14395/14395 [==============================] - 59s 4ms/sample - loss: 0.3034\n",
      "Test res 0.8855994152046783 0.551547802605928 0.527643064985451\n",
      "Repeat 1 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4826val_aucs: 0.543950337213183 0.8774031445121493\n",
      "14395/14395 [==============================] - 82s 6ms/sample - loss: 0.4823\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4406val_aucs: 0.5563234849772317 0.8786007673211466\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4405\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4247val_aucs: 0.5440820968742068 0.876335665387798\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4248\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4082val_aucs: 0.5412243722811706 0.8760151959678026\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4082\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3965val_aucs: 0.5482952716733921 0.875818099751749\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3963\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3861val_aucs: 0.4882301407049726 0.8608895659369593\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3860\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3751val_aucs: 0.5376768157968573 0.8767900398706087\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3754\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3661val_aucs: 0.5218717957227229 0.8747784548258482\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3656\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3505val_aucs: 0.49232551666618524 0.8681554201459415\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3505\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3454val_aucs: 0.4671077624584493 0.8555186940494998\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3454\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3342val_aucs: 0.4632508295186194 0.8532453170841795\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3341\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3212val_aucs: 0.4693069183805798 0.8580801925825623\n",
      "14395/14395 [==============================] - 59s 4ms/sample - loss: 0.3221\n",
      "Test res 0.8889970050217523 0.5649430729464112 0.5419103313840156\n",
      "Repeat 2 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4898val_aucs: 0.5344809410162149 0.8758792178747021\n",
      "14395/14395 [==============================] - 83s 6ms/sample - loss: 0.4896\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4391val_aucs: 0.5408004907929927 0.8727417218137817\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4389\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4287val_aucs: 0.5288036933884276 0.8717129718864445\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4288\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4103val_aucs: 0.5353861789724743 0.8773087595952922\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4101\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4022val_aucs: 0.5229772118614556 0.8764322187650107\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4018\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3881val_aucs: 0.5205977685589908 0.8738676102100945\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3897\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3742val_aucs: 0.5038536146380217 0.8679666394373005\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3737\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3646val_aucs: 0.507093851210729 0.8657217464731851\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3644\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3554val_aucs: 0.5084766888567979 0.8682458169268754\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3557\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3436val_aucs: 0.5054075519387373 0.8657240410826883\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3433\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3330val_aucs: 0.47405856794324763 0.8532742547873202\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3331\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3228val_aucs: 0.4768879921665701 0.8560277861913459\n",
      "14395/14395 [==============================] - 59s 4ms/sample - loss: 0.3229\n",
      "Test res 0.8884443838696192 0.5696615723672748 0.5350877192982456\n",
      "Repeat 3 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4905val_aucs: 0.5159452400779095 0.8672428243420771\n",
      "14395/14395 [==============================] - 84s 6ms/sample - loss: 0.4901\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4422val_aucs: 0.5142817864447942 0.8730192143517057\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4420\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4188val_aucs: 0.5408506658898806 0.8767451379386966\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4184\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4074val_aucs: 0.5144456369158759 0.8693767127826701\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4069\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3910val_aucs: 0.5352572476006044 0.8765837514179794\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3909\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3821val_aucs: 0.5243422672806887 0.8695357603683044\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3827\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3663val_aucs: 0.4896145151332687 0.8617120225161484\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3663\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3573val_aucs: 0.5295736423914184 0.8747492466679921\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3577\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3455val_aucs: 0.49794819297773646 0.8625010232840987\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3451\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3398val_aucs: 0.48123052131702704 0.8625610559512255\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3396\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3292val_aucs: 0.5133066820226734 0.8660063073282734\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3292\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3176val_aucs: 0.4827061990872892 0.8600467007371543\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3176\n",
      "Epoch 13/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3089val_aucs: 0.44070145409114975 0.8513497604540652\n",
      "14395/14395 [==============================] - 61s 4ms/sample - loss: 0.3090\n",
      "Test res 0.8878830737056598 0.5642656985129922 0.5370370370370371\n",
      "Repeat 4 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4766val_aucs: 0.5255651200602012 0.8704428026461924\n",
      "14395/14395 [==============================] - 86s 6ms/sample - loss: 0.4767\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4327val_aucs: 0.5264524379854482 0.8739233842811998\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4323\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4155val_aucs: 0.5331361521538113 0.8733377942417491\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4154\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4054val_aucs: 0.5351230428760902 0.8723536350654996\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4051\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3882val_aucs: 0.5167567602305926 0.8673776526883872\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3884\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3805val_aucs: 0.5340833972614593 0.8727751985755752\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3802\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3685val_aucs: 0.5119325018199856 0.8656898656898656\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3686\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3530val_aucs: 0.5037405205675888 0.8561472007799691\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3527\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3507val_aucs: 0.4547746518923845 0.851741478860123\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3509\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3365val_aucs: 0.4895956383191329 0.8551216517318212\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3362\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3288val_aucs: 0.48673051938028644 0.8516571661581078\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3287\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3218val_aucs: 0.4702759582759385 0.8510259706116581\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3219\n",
      "Epoch 13/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3084val_aucs: 0.4658743170163763 0.8492649848582052\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3085\n",
      "Epoch 14/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3031val_aucs: 0.4577949220496257 0.8470935495417603\n",
      "14395/14395 [==============================] - 61s 4ms/sample - loss: 0.3029\n",
      "Test res 0.8861946745791291 0.5659701034061162 0.5487329434697856\n",
      "Repeat 5 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4877val_aucs: 0.5110159431284352 0.8653765364955298\n",
      "14395/14395 [==============================] - 86s 6ms/sample - loss: 0.4876\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4329val_aucs: 0.5242025781727861 0.868384619868035\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4328\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4173val_aucs: 0.5189873801862624 0.867818428030432\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4168\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4079val_aucs: 0.5082199735448034 0.8615619325308244\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4077\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3949val_aucs: 0.5336940868266161 0.8679425663477675\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3946\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3823val_aucs: 0.4985901253851405 0.857945647097472\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3824\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3739val_aucs: 0.4979620902396141 0.858758601687827\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3749\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3596val_aucs: 0.5185297537951842 0.863652224990103\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3597\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3512val_aucs: 0.5008903418511894 0.855790636186168\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3510\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3375val_aucs: 0.4831337188590384 0.848467989345299\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3389\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3265val_aucs: 0.4600803065292356 0.8423836979139465\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3267\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3233val_aucs: 0.47727159067021724 0.8499712740905923\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3238\n",
      "Epoch 13/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3102val_aucs: 0.47336473981161076 0.8434479569028095\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3102\n",
      "Epoch 14/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3007val_aucs: 0.45269025183954825 0.8416543852996009\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3008\n",
      "Epoch 15/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.2871val_aucs: 0.45403577901700376 0.8360144182113939\n",
      "14395/14395 [==============================] - 61s 4ms/sample - loss: 0.2873\n",
      "Test res 0.8908142497807887 0.5731671578653748 0.5402521823472357\n",
      "Repeat 6 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4734val_aucs: 0.4983049007570376 0.8723578343420537\n",
      "14395/14395 [==============================] - 86s 6ms/sample - loss: 0.4743\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4289val_aucs: 0.5140210774886602 0.8727142154496984\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4290\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4187val_aucs: 0.527836000312302 0.8762807205517982\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.4188\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4069val_aucs: 0.5223321632449445 0.8773002168305789\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4064\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3923val_aucs: 0.5039369996366373 0.8705451399392382\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3921\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3858val_aucs: 0.5131964985832828 0.8718390419736894\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3857\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3695val_aucs: 0.4853093462436185 0.8609705727729452\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3691\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3619val_aucs: 0.48870182923878946 0.8622152126240502\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3618\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3463val_aucs: 0.48813685967490433 0.8621436285138755\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3462\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3437val_aucs: 0.4705902155545657 0.8607827606989379\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3435\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3278val_aucs: 0.48750766778883803 0.8561274843727269\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3280\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3158val_aucs: 0.48959223014448916 0.8575191410522556\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3157\n",
      "Epoch 13/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3049val_aucs: 0.4445632735456545 0.8386748164791239\n",
      "14395/14395 [==============================] - 61s 4ms/sample - loss: 0.3052\n",
      "Test res 0.8899967378967031 0.5707562373449316 0.5413022351797862\n",
      "Repeat 7 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4911val_aucs: 0.5287684994163403 0.8694367192591165\n",
      "14395/14395 [==============================] - 87s 6ms/sample - loss: 0.4908\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4440val_aucs: 0.5387899241742059 0.8763450236038831\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4439\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4221val_aucs: 0.5453743777006097 0.8763419641245452\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4219\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4071val_aucs: 0.5372934729847092 0.8748856519597495\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4068\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4024val_aucs: 0.5435950342736567 0.8771481369300572\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4020\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3917val_aucs: 0.5213197429406253 0.8691185334079845\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3928\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3840val_aucs: 0.5236639139585417 0.8700317879903197\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3844\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3732val_aucs: 0.5222958375392595 0.8734507561503183\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3731\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3621val_aucs: 0.48366111239419524 0.8619991249889093\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3619\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3474val_aucs: 0.5002074005843838 0.8674006357598064\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3472\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3360val_aucs: 0.49211082231853626 0.8663313477312432\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3365\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3243val_aucs: 0.4883654106052723 0.8616434605158894\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3242\n",
      "Epoch 13/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3152val_aucs: 0.4849430665783201 0.8630294046559155\n",
      "14395/14395 [==============================] - 61s 4ms/sample - loss: 0.3151\n",
      "Test res 0.8907536749554874 0.5695798457564486 0.543859649122807\n",
      "Repeat 8 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4746val_aucs: 0.5242728139334487 0.8755646059921214\n",
      "14395/14395 [==============================] - 88s 6ms/sample - loss: 0.4742\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4332val_aucs: 0.54411331249201 0.8808881460206851\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4332\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4131val_aucs: 0.5351057343697656 0.8743184707377937\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4138\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4052val_aucs: 0.5318325075719403 0.8784186323900999\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4048\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3930val_aucs: 0.544150668588347 0.8756187516675299\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3934\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3863val_aucs: 0.5278659226146385 0.8762025832980209\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3861\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3771val_aucs: 0.5302979707651302 0.8745107271214904\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3767\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3637val_aucs: 0.5134876607197586 0.8665599447557166\n",
      "14395/14395 [==============================] - 51s 4ms/sample - loss: 0.3639\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3469val_aucs: 0.4686765261828483 0.8474845802533076\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3466\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3431val_aucs: 0.5187165333677216 0.8661903416670591\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3430\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3302val_aucs: 0.5060393153796775 0.8634014470235573\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3299\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3121val_aucs: 0.4700933412468577 0.8547067501608675\n",
      "14395/14395 [==============================] - 61s 4ms/sample - loss: 0.3122\n",
      "Test res 0.8908459026095833 0.5642918044539654 0.5341130604288499\n",
      "Repeat 9 ld 50\n",
      "Num train: 14395 Num valid: 3572\n",
      "new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_50ld.h5\n",
      "Train on 14395 samples\n",
      "Epoch 1/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4906val_aucs: 0.5220111967148982 0.8743675774782536\n",
      "14395/14395 [==============================] - 89s 6ms/sample - loss: 0.4907\n",
      "Epoch 2/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4336val_aucs: 0.5368730864637374 0.8770215776060272\n",
      "14395/14395 [==============================] - 53s 4ms/sample - loss: 0.4332\n",
      "Epoch 3/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4153val_aucs: 0.5429881133059179 0.8795909083561749\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4158\n",
      "Epoch 4/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.4072val_aucs: 0.5400628744667629 0.8792999535857866\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.4071\n",
      "Epoch 5/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3964val_aucs: 0.5338972686457564 0.8791798770138805\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3963\n",
      "Epoch 6/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3861val_aucs: 0.53665045163764 0.875791562465603\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3858\n",
      "Epoch 7/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3761val_aucs: 0.5220045523354818 0.8706590587382261\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3763\n",
      "Epoch 8/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3629val_aucs: 0.5247312861835278 0.8764458258381691\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3626\n",
      "Epoch 9/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3547val_aucs: 0.5116721535741712 0.8752088832032119\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3545\n",
      "Epoch 10/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3437val_aucs: 0.4763003135070936 0.8680381566398879\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3433\n",
      "Epoch 11/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3322val_aucs: 0.4931971215082704 0.8629441389932504\n",
      "14395/14395 [==============================] - 52s 4ms/sample - loss: 0.3320\n",
      "Epoch 12/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3256val_aucs: 0.4963792728255551 0.871666624331978\n",
      "14395/14395 [==============================] - 53s 4ms/sample - loss: 0.3255\n",
      "Epoch 13/1000\n",
      "14368/14395 [============================>.] - ETA: 0s - loss: 0.3079val_aucs: 0.4909209098967945 0.8677387349317371\n",
      "14395/14395 [==============================] - 62s 4ms/sample - loss: 0.3077\n",
      "Test res 0.889354247536541 0.5732868556309977 0.5389863547758285\n",
      "gen_res {10: [(0.8734300631368426, 0.006136223098497274), (0.5248239434565196, 0.014754366403570052), (0.5043583696473467, 0.01070374615713679)], 20: [(0.8805549287054166, 0.004135513636745678), (0.5407721696753323, 0.011595141675669765), (0.5163142144652165, 0.010358914479794616)], 30: [(0.8853571841742806, 0.0035227240740787163), (0.5586857088039603, 0.010818887784918586), (0.5313213842746036, 0.008174419170316733)], 40: [(0.8875623808984734, 0.0027047954157572197), (0.5619564794838035, 0.01057049196019184), (0.5298354984819416, 0.0057264861003417005)], 50: [(0.8888883365159941, 0.001782094140170721), (0.566747015089044, 0.006030257330621745), (0.5388924578029043, 0.005528024082183693)]}\n"
     ]
    }
   ],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "lds = [10,20,30,40,50]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore.h5'\n",
    "f = open('log.csv', 'a+')\n",
    "f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2021)\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "        f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "infinite-theology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x= range(10,51,10)\n",
    "for i,metric in enumerate(['ROC-AUC', 'PR-AUC', 'min(Re,Pr)']):\n",
    "    plt.figure()\n",
    "    y = [gen_res[ld][i][0] for ld in x]\n",
    "    plt.plot(x, y, color='r', marker='^')\n",
    "    plt.xlabel('% labeled data')\n",
    "    plt.ylabel(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-charm",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-mouth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
